[{"authors":null,"categories":null,"content":"I am a senior year undergraduate student at BITS Pilani (Goa Campus), pursuing a major in Electrical \u0026amp; Electronics engineering. I spend most of my time dabbling in research related to computational models of language. Currently, I am working as a Research Assistant at MIDAS-IIITD, and as a Project Assistant at the Cognitive Neuroscience Lab at BITS Goa. I also play around with competitive Data Science and am currently a Competitions Expert on Kaggle. I am also active in the student community: leading the students' Language Research Group (LRG) and being a core member at the Society for Artificial Intelligence and Deep Learning (SAiDL) at BITS Goa.\nApart from the above mentioned technical interests, I also share a keen interest towards Social Sciences, Eco-criticism, Electronic Music \u0026amp; Table Tennis.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://rajaswa.github.io/author/rajaswa-patil/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rajaswa-patil/","section":"authors","summary":"I am a senior year undergraduate student at BITS Pilani (Goa Campus), pursuing a major in Electrical \u0026amp; Electronics engineering. I spend most of my time dabbling in research related to computational models of language.","tags":null,"title":"Rajaswa Patil","type":"authors"},{"authors":null,"categories":null,"content":"I\u0026rsquo;ll be attending COLING 2020 to present our work on Humor Grading and Counterfactual Detection! Shoot me an e-mail or catch me at the conference socials!\n","date":1607126400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607126400,"objectID":"128302e92f11e87e0c089e445661729b","permalink":"https://rajaswa.github.io/news/coling2020/","publishdate":"2020-12-05T00:00:00Z","relpermalink":"/news/coling2020/","section":"news","summary":"I\u0026rsquo;ll be attending COLING 2020 to present our work on Humor Grading and Counterfactual Detection! Shoot me an e-mail or catch me at the conference socials!","tags":null,"title":"I'll be attending COLING 2020 to present our work on Humor Grading and Counterfactual Detection!","type":"news"},{"authors":["Rajaswa Patil*","Somesh Singh*","Swati Agarwal"],"categories":null,"content":"","date":1606761000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606761000,"objectID":"af19c65db8acdb4040658ec5c51bcab3","permalink":"https://rajaswa.github.io/publication/semeval-propaganda-2020/","publishdate":"2020-12-01T00:00:00+05:30","relpermalink":"/publication/semeval-propaganda-2020/","section":"publication","summary":"Propaganda spreads the ideology and beliefs of like-minded people, brainwashing their audiences, and sometimes leading to violence. SemEval 2020 Task-11 aims to design automated systems for news propaganda detection. Task-11 consists of two sub-tasks, namely, Span Identification - given any news article, the system tags those specific fragments which contain at least one propaganda technique; and Technique Classification - correctly classify a given propagandist statement amongst 14 propaganda techniques. For sub-task 1, we use contextual embeddings extracted from pre-trained transformer models to represent the text data at various granularities and propose a multi-granularity knowledge sharing approach. For sub-task 2, we use an ensemble of BERT and logistic regression classifiers with linguistic features. Our results reveal that the linguistic features are the strong indicators for covering minority classes in a highly imbalanced dataset.","tags":["NLP","AI4SG","Social Computing","Information Retrieval"],"title":"BPGC at SemEval-2020 Task 11: Propaganda Detection in News Articles with Multi-Granularity Knowledge Sharing and Linguistic Features Based Ensemble Learning","type":"publication"},{"authors":["Rajaswa Patil","Veeky Baths"],"categories":null,"content":"","date":1606761000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606761000,"objectID":"0a706038cb88bb531f6631dc4ad97aea","permalink":"https://rajaswa.github.io/publication/semeval-counterfactual-2020/","publishdate":"2020-12-01T00:00:00+05:30","relpermalink":"/publication/semeval-counterfactual-2020/","section":"publication","summary":"In this paper, we describe an approach for modelling causal reasoning in natural language by detecting counterfactuals in text using multi-head self-attention weights. We use pre-trained transformer models to extract contextual embeddings and self-attention weights from the text. We show the use of convolutional layers to extract task-specific features from these self-attention weights. Further, we describe a fine-tuning approach with a common base model for knowledge sharing between the two closely related sub-tasks for counterfactual detection. We analyze and compare the performance of various transformer models in our experiments. Finally, we perform a qualitative analysis with the multi-head self-attention weights to interpret our models’ dynamics.","tags":["NLP","Information Retrieval"],"title":"CNRL at SemEval-2020 Task 5: Modelling Causal Reasoning in Language with Multi-Head Self-Attention Weights Based Counterfactual Detection","type":"publication"},{"authors":["Siddhant Mahurkar","Rajaswa Patil"],"categories":null,"content":"","date":1606761000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606761000,"objectID":"d2a91774b7bad44c659dc1eb7d4ae81a","permalink":"https://rajaswa.github.io/publication/semeval-humor-2020/","publishdate":"2020-12-01T00:00:00+05:30","relpermalink":"/publication/semeval-humor-2020/","section":"publication","summary":"In this paper, we assess the ability of BERT and its derivative models (RoBERTa, DistilBERT, and ALBERT) for short-edits based humor grading. We test these models for humor grading and classification tasks on the Humicroedit and the FunLines dataset. We perform extensive experiments with these models to test their language modeling and generalization abilities via zero-shot inference and cross-dataset inference based approaches. Further, we also inspect the role of self-attention layers in humor-grading by performing a qualitative analysis over the self-attention weights from the final layer of the trained BERT model. Our experiments show that all the pre-trained BERT derivative models show significant generalization capabilities for humor-grading related tasks.","tags":["NLP"],"title":"LRG at SemEval-2020 Task 7: Assessing the Ability of BERT and Derivative Models to Perform Short-Edits Based Humor Grading","type":"publication"},{"authors":["Rajaswa Patil","Siddhant Mahurkar"],"categories":null,"content":"","date":1576089000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576089000,"objectID":"97a43b49ee45dd2de71b81525aaebcfa","permalink":"https://rajaswa.github.io/publication/kedl-citta-2019/","publishdate":"2019-12-12T00:00:00+05:30","relpermalink":"/publication/kedl-citta-2019/","section":"publication","summary":"Most of the recommendation and search frameworks in Digital Libraries follow a keyword-based approach to resolve text-based search queries. Keyword-based methods usually fail to capture the semantic aspects of the user’s query and often lead to a misleading set of results. In this work, we propose an efficient and content-sentiment aware semantic recommendation framework, Citta. The framework is designed with the BERT language model. It is designed to retrieve semantically related reading recommendations with short input queries and shorter response times. We test the proposed framework on the CMU Book Summary Dataset and discuss the observed advantages and shortcomings of the framework.","tags":["NLP","Information Retrieval"],"title":"Citta: A Lite Semantic Recommendation Framework for Digital Libraries","type":"publication"},{"authors":["Ajay Subramanian","Rajaswa Patil","Veeky Baths"],"categories":null,"content":"","date":1576089000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576089000,"objectID":"e60e9dd094f8f8cd0fe3294c4ae4a18f","permalink":"https://rajaswa.github.io/publication/accs-word2brain2image-2019/","publishdate":"2019-12-12T00:00:00+05:30","relpermalink":"/publication/accs-word2brain2image-2019/","section":"publication","summary":"Recent work in cognitive neuroscience has aimed to better understand how the brain responds to external stimuli. Extensive study is being done to gauge the involvement of various regions of the brain in the processing of external stimuli. A study by Ostarek et al. has produced experimental evidence of the involvement of low-level visual representations in spoken word processing, using Continuous Flash Suppression (CFS). For example, hearing the word ‘car’ induces a visual representation of a car in extrastriate areas of the visual cortex that seems to have a spatial resolution of some kind. Though the structure of these areas of the brain has been extensively studied, research hasn’t really delved into the functional aspects. In this work, we aim to take this a step further by experimenting with generative models such as Variational Autoencoders (VAEs) (Kingma et al 2013) and Generative Adversarial Networks (GANs) (Goodfellow et al. 2014) to generate images purely from the EEG signals induced by listening to spoken words of objects.","tags":["Cognitive Science","Neuroscience"],"title":"Word2Brain2Image: Visual Reconstruction from Spoken Word Representations","type":"publication"}]