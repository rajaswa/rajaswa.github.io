[{"authors":null,"categories":null,"content":"I am a senior year undergraduate student at BITS Pilani (Goa Campus), pursuing a major in Electrical \u0026amp; Electronics engineering. I spend most of my time dabbling in research related to computational models of language. Currently, I am working as a Research Assistant at MIDAS-IIITD, and as a Project Assistant at the Cognitive Neuroscience Lab at BITS Goa. I also play around with competitive Data Science and am currently a Competitions Expert on Kaggle. I am also active in the student community: delivering guidance talks, leading research groups and mentoring projects.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://rajaswa.github.io/author/rajaswa-patil/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rajaswa-patil/","section":"authors","summary":"I am a senior year undergraduate student at BITS Pilani (Goa Campus), pursuing a major in Electrical \u0026amp; Electronics engineering. I spend most of my time dabbling in research related to computational models of language.","tags":null,"title":"Rajaswa Patil","type":"authors"},{"authors":["Rajaswa Patil*","Somesh Singh*","Swati Agarwal"],"categories":null,"content":"","date":1606761000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606761000,"objectID":"a8b3a9b1b526afe566cc5e9bb79df1d7","permalink":"https://rajaswa.github.io/publication/bpgc-semeval-propaganda/","publishdate":"2020-12-01T00:00:00+05:30","relpermalink":"/publication/bpgc-semeval-propaganda/","section":"publication","summary":"Propaganda spreads the ideology and beliefs of like-minded people, brainwashing their audiences, and sometimes leading to violence. SemEval 2020 Task-11 aims to design automated systems for news propaganda detection. Task-11 consists of two sub-tasks, namely, Span Identification - given any news article, the system tags those specific fragments which contain at least one propaganda technique; and Technique Classification - correctly classify a given propagandist statement amongst 14 propaganda techniques. For sub-task 1, we use contextual embeddings extracted from pre-trained transformer models to represent the text data at various granularities and propose a multi-granularity knowledge sharing approach. For sub-task 2, we use an ensemble of BERT and logistic regression classifiers with linguistic features. Our results reveal that the linguistic features are the strong indicators for covering minority classes in a highly imbalanced dataset.","tags":["NLP","AI4SG","Social Computing"],"title":"BPGC at SemEval-2020 Task 11: Propaganda Detection in News Articles with Multi-Granularity Knowledge Sharing and Linguistic Features Based Ensemble Learning","type":"publication"},{"authors":["Rajaswa Patil","Veeky Baths"],"categories":null,"content":"","date":1606761000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606761000,"objectID":"3ae4492dacb5767e2137a02c338a5ada","permalink":"https://rajaswa.github.io/publication/cnrl-semeval-counterfactual/","publishdate":"2020-12-01T00:00:00+05:30","relpermalink":"/publication/cnrl-semeval-counterfactual/","section":"publication","summary":"In this paper, we describe an approach for modelling causal reasoning in natural language by detecting counterfactuals in text using multi-head self-attention weights. We use pre-trained transformer models to extract contextual embeddings and self-attention weights from the text. We show the use of convolutional layers to extract task-specific features from these self-attention weights. Further, we describe a fine-tuning approach with a common base model for knowledge sharing between the two closely related sub-tasks for counterfactual detection. We analyze and compare the performance of various transformer models in our experiments. Finally, we perform a qualitative analysis with the multi-head self-attention weights to interpret our modelsâ€™ dynamics.","tags":["NLP"],"title":"CNRL at SemEval-2020 Task 5: Modelling Causal Reasoning in Language with Multi-Head Self-Attention Weights Based Counterfactual Detection","type":"publication"}]