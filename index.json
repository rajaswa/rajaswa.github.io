[{"authors":null,"categories":null,"content":"I am a senior year undergraduate student at BITS Pilani (Goa Campus), pursuing a major in Electrical \u0026amp; Electronics engineering. I spend most of my time dabbling in research related to computational models of language. Currently, I am working as a Research Assistant at MIDAS-IIITD, and as a Project Assistant at the Cognitive Neuroscience Lab at BITS Goa. I also play around with competitive Data Science and am currently a Competitions Expert on Kaggle.\nI am also active in the student community: leading the students' Language Research Group (LRG) and being a core member at the Society for Artificial Intelligence and Deep Learning (SAiDL) at BITS Goa. Apart from the above mentioned technical interests, I also share a keen interest towards Social Sciences, Eco-criticism, Electronic Music \u0026amp; Table Tennis.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://rajaswa.github.io/author/rajaswa-patil/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rajaswa-patil/","section":"authors","summary":"I am a senior year undergraduate student at BITS Pilani (Goa Campus), pursuing a major in Electrical \u0026amp; Electronics engineering. I spend most of my time dabbling in research related to computational models of language.","tags":null,"title":"Rajaswa Patil","type":"authors"},{"authors":null,"categories":null,"content":"Our paper Vyākarana: A Colorless Green Benchmark for Syntactic Evaluation in Indic Languages has been accpeted for presentation at the ACL-IJCNLP Student Research Workshop, 2021.\nThis work was funded by the Cognitive Neuroscience Lab at BITS Goa, and mentored by Prof. Greg Durrett, UT Austin during the pre-submission mentoring stage at the workshop.\n","date":1620950400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620950400,"objectID":"cc193d1422c825ea022b2e011478a667","permalink":"https://rajaswa.github.io/news/aclsrw2021accept/","publishdate":"2021-05-14T00:00:00Z","relpermalink":"/news/aclsrw2021accept/","section":"news","summary":"Our paper Vyākarana: A Colorless Green Benchmark for Syntactic Evaluation in Indic Languages has been accpeted for presentation at the ACL-IJCNLP Student Research Workshop, 2021.\nThis work was funded by the Cognitive Neuroscience Lab at BITS Goa, and mentored by Prof.","tags":null,"title":"Paper on Syntactic Evaluation in Indic languages accepted at ACL-IJCNLP SRW 2021!","type":"news"},{"authors":null,"categories":null,"content":"I\u0026rsquo;ll be attending the Fifth Summer School on Statistical Methods for Linguistics and Psychology (Foundational methods in frequentist statistics track - class of 30 students), 6-10 September 2021. Shoot me an e-mail or catch me at the summer school!\nThis summer school is organized by Prof. Shravan Vasishth, University of Potsdam.\n","date":1620604800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620604800,"objectID":"59a86b1692b264bd1e2f67c052342fd6","permalink":"https://rajaswa.github.io/news/smlp2021/","publishdate":"2021-05-10T00:00:00Z","relpermalink":"/news/smlp2021/","section":"news","summary":"I\u0026rsquo;ll be attending the Fifth Summer School on Statistical Methods for Linguistics and Psychology (Foundational methods in frequentist statistics track - class of 30 students), 6-10 September 2021. Shoot me an e-mail or catch me at the summer school!","tags":null,"title":"I'll be attending the Fifth Summer School on Statistical Methods for Linguistics and Psychology, 6-10 September 2021!","type":"news"},{"authors":null,"categories":null,"content":"I\u0026rsquo;ll be contributing to a tutorial on Transformer models for NLP at the Neuromatch DL Summer School. This lecture will be taught by Prof. He He, NYU.\nDo sign-up for the summer school if interested in an Introduction to Deep Learning!\n","date":1620604800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620604800,"objectID":"fbc7d5e3beb76ae20470b6fc84ad21c5","permalink":"https://rajaswa.github.io/news/neuromatch2021/","publishdate":"2021-05-10T00:00:00Z","relpermalink":"/news/neuromatch2021/","section":"news","summary":"I\u0026rsquo;ll be contributing to a tutorial on Transformer models for NLP at the Neuromatch DL Summer School. This lecture will be taught by Prof. He He, NYU.\nDo sign-up for the summer school if interested in an Introduction to Deep Learning!","tags":null,"title":"I'll be contributing to a tutorial on Transformer models at Neuromatch DL Summer School!","type":"news"},{"authors":null,"categories":null,"content":"My final project submission for the Meta Learning course at BITS Goa (conducted by TCS Research \u0026amp; BITS Goa). The project is based on the Feedback Transformer paper. The paper introduces a feedback mechanism in transformer models by adding a recurrent memory-attention based approach. This helps the transformer model in:\n Accessing higher level (layers) representations Maintaining a belief state Perform a learnable wieghted combined top-down and bottom-up processing Decrease compute memory-consumption at inference  The project can be run as a colab notebook  , where the approach given in the paper can be understood in more detail through experimentation. The experiments' Tensorboard logs and a video explanation for the notebook can be found here.\nKey Contributions The key contributions of this project can be listed as follows:\n Implementing and Open-sourcing a modular customizable Feedback Transformer Model in PyTorch Experimenting the Feedback Transformer Model with COGS Benchmark (Compositional Generalization) Implementing the Sequence Copy \u0026amp; Reverse Task from the original Feedback Transformer Paper  Feedback Transformer Implementation The Feedback Transformer Model has been implemented as PyTorch model class in the given notebook. You can adjust the various hyperparameters and turn the feedback ON/OFF in the Encoder and Decoder of the Model independently. Use the model in the following manner:\nmodel = FeedbackTransformerModel(\rencoder_feedback = False, # Disable Feedback Mehancism in the Encoder\rdecoder_feedback = True, # Enable Feedback Mehancism in the Decoder\rmemory_context = 8, # How long to look in the past for Memory-attention\rinput_vocab_size = 800, # Input Vocabulary Size\routput_vocab_size = 800, # Output Vocabulary Size\rd_model = 128, # Model Embedding Dimension\rnhead = 8, # Number of Heads in Multi-head Cross-attention and Memory-attention\rnum_layers = 4, # Number of Encoder and Decoder blocks\rdim_feedforward = 256, # Feedforward Dimension\rmax_seq_length = 1000, # Maximum Sequence Length in Data\rdropout = 0.1, # Model Dropout Probability PAD_IDX = 0, # PAD Token ID to Mask Padding tokens for Attention\ractivation = \u0026quot;gelu\u0026quot;, # Model Activation Function: \u0026quot;gelu\u0026quot; / \u0026quot;relu\u0026quot;\r)\r Solving COGS with Feedback Transformer The COGS Benchmark is a benchmark for evaluating compositional generalization \u0026amp; reasoning in natural language. The COGS task is that of mapping a natural language sentence to a lambda-expression based semantic logical form:\ninput_sentence = \u0026quot;The moose wanted to read .\u0026quot;\routput_logical_form = \u0026quot;* moose ( x _ 1 ) ; want . agent ( x _ 2 , x _ 1 ) AND want . xcomp ( x _ 2 , x _ 4 ) AND read . agent ( x _ 4 , x _ 1 )\u0026quot;\r This can be treated as a sequence-to-sequence semantic-parsing task. What makes this task challenging is its Generalization test set. The following points make it quite challenging:\n Novel (unseen in training) Combination of Familiar Primitives and Grammatical Roles Novel (unseen in training) Combination Modified Phrases and Grammatical Roles Deeper Recursion (results in longer sentences and deeper lingusitic strucutre i.e. parse tree) Verb Argument Structure Alternation Verb Class Alteration  You can check the COGS Paper for more details on the benchmark.\nThe COGS dataset can be loaded as a PyTorch-Lightning Module in the following manner:\ndatamodule = COGSDataModule(\rbatch_size=128, # Batch Size for Training num_workers=2, # Number of workers for Data Loading\ruse_100=False, # Whether to use single-exposure or hundred-exposures for pimitives in the training set\ruse_Gen=True # Whether to use normal test set or generaliztion test set\r)\r NOTE: The feedback transformer paper does not include this benchmark or any related task. This is the first attempt (to the best of my knowledge) to inspect the effect of incoroporating feedback and memory based architectural biases in solving compositional generalization problem in natural language.\nResults While the PyTorch-Lightning profiler and Tensorboard logger (included in the notebook) will give a detailed insights into the experiments, here are key metrics to report:\n   Encoder Feedback Decoder Feedback Num Parameters Validation Accuracy Generalization Accuracy Total Training time Mean Forward time Mean Backward time Inference time     False False 12.7k 69.5% 65.44% 193.43 s 22.58 ms 25.17 ms 20.08 ms   False True 12.3k 74.1% 70.86% 4441.7 s 645.08 ms 1039.30 ms 365.49 ms   True True 12.2k 74.4% 70.69% 7402.4 s 701.85 ms 1129.4 ms 404.65 ms    NOTE: The results are subject to change in hyperparameters and training settings. The above results are obtained from the current settings given in the notebook. The results can be increased significantly by training bigger models for longer times.\nDiscussion  The Validation accuracy (roughly equal to the Normal test accuracy) reflects the Expressivity of the models towards the COGS task  Access to higher level representations might help in semantic-parsing by allowing top-down processing In general, incorporating feedback gives the model more expressivity with lesser number of parameters   The Generalization test accuracy (usually lower than Validation and Normal test accuracy) reflects the Compositional Generalization capabilities of the models  This needs accurate inference on previously unseen novel linguistic structures and an ability to maintain a belief state for longer contexts On an absolute scale, incorporating feedback increases the Generalization test accuracies significantly High Expressivity can lead to poor Compositional Generalization in Vanilla Transformer models (as reported in the COGS Paper) The Vanilla Transformer model (no feedback) shows a 5.84% decrease in accuracy between the Validation and Generalization test set Enabling feedback in Decoder reduces the drop in Generalization accuracy to 4.37% Enabling feedback in Encoder further reduces the the drop in Generalization accuracy to 4.98%    Sequence Copy \u0026amp; Reverse Task The Sequence Copy \u0026amp; Reverse task is included in the Feedback Transformer paper as an Algorithmic task to test the role of memory in long-sequence processing. Since the official dataset is not publicly available, we generate the dataset synthetically.\nThe sequence copy \u0026amp; reverse dataset can be loaded as a PyTorch-Lightning Module in the following manner:\ndatamodule = SequenceCopyDataModule(\rbatch_size=64, # Batch Size for Training\rnum_workers=2, # Number of workers for Data Loading\rnum_samples_train=10000, # Number of samples to generate for training set\rnum_samples_eval=1000, # Number of samples to generate for validation and test set\rmax_length_train=10, # Sequence length in training samples\rmax_length_eval=50, # Sequence length in evaluation samples (Should be significantly longer to test for memory effect)\rreverse=True, # Whether to Copy the Input Sequence or Reverse the Input Sequence\r)\r NOTE: The ablation analysis for this task with Feedback Transformer is still in progress. One can still train the Feedback Transformer for this task using the last section of the project\u0026rsquo;s colab notebook.\nCitations If you use the code in this repository in any manner, cite the repository:\n@misc{patil2021-feedback-github,\rauthor = {Rajaswa Patil},\rtitle = {feedback-and-memory-in-transformers},\rmonth = apr,\ryear = 2021,\rpublisher = {Github},\rurl = \u0026quot;https://github.com/rajaswa/feedback-and-memory-in-transformers\u0026quot;\r}\r If you use the code for Feedback Transfomer or the Sequence Copy \u0026amp; Reverse task, cite the Feedback Transformer paper:\n@misc{fan2021addressing,\rtitle={Addressing Some Limitations of Transformers with Feedback Memory}, author={Angela Fan and Thibaut Lavril and Edouard Grave and Armand Joulin and Sainbayar Sukhbaatar},\ryear={2021},\reprint={2002.09402},\rarchivePrefix={arXiv},\rprimaryClass={cs.LG}\r}\r If you use the code from COGS Benchmark data processing and loading, cite the COGS paper:\n@inproceedings{kim-linzen-2020-cogs,\rtitle = \u0026quot;{COGS}: A Compositional Generalization Challenge Based on Semantic Interpretation\u0026quot;,\rauthor = \u0026quot;Kim, Najoung and\rLinzen, Tal\u0026quot;,\rbooktitle = \u0026quot;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\u0026quot;,\rmonth = nov,\ryear = \u0026quot;2020\u0026quot;,\raddress = \u0026quot;Online\u0026quot;,\rpublisher = \u0026quot;Association for Computational Linguistics\u0026quot;,\rurl = \u0026quot;https://www.aclweb.org/anthology/2020.emnlp-main.731\u0026quot;,\rdoi = \u0026quot;10.18653/v1/2020.emnlp-main.731\u0026quot;,\rpages = \u0026quot;9087--9105\u0026quot;,\rabstract = \u0026quot;Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96{--}99{\\%}), but generalization accuracy was substantially lower (16{--}35{\\%}) and showed high sensitivity to random seed (+-6{--}8{\\%}). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.\u0026quot;,\r}\r Contact Submit an issue.\n","date":1619913600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619913600,"objectID":"eb07a5358eea9ed309346a630c4b83d1","permalink":"https://rajaswa.github.io/project/feedback-and-memory-in-transformers/","publishdate":"2021-05-02T00:00:00Z","relpermalink":"/project/feedback-and-memory-in-transformers/","section":"project","summary":"My final project submission for the Meta Learning course at BITS Goa (conducted by TCS Research \u0026 BITS Goa). The project is based on the Feedback Transformer paper.","tags":["Transformer","Generalization","Cognitive Science","NLP"],"title":"Feedback and Memory in Transformers","type":"project"},{"authors":["Arijit Gupta","Rajaswa Patil","Veeky Baths"],"categories":null,"content":"","date":1615573800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615573800,"objectID":"cba659156a4dd9094e08e8c110f5354e","permalink":"https://rajaswa.github.io/publication/lexical-development-children/","publishdate":"2021-03-13T00:00:00+05:30","relpermalink":"/publication/lexical-development-children/","section":"publication","summary":"Recent work has shown that distributed word representations can encode abstract semantic and syntactic information from child-directed speech. In this paper, we use diachronic distributed word representations to perform temporal modeling and analysis of lexical development in children. Unlike all previous work, we use temporally sliced speech corpus to learn distributed word representations of child and child-directed speech. Through our modeling experiments, we demonstrate the dynamics of growing lexical knowledge in children over time, as compared against a saturated level of lexical knowledge in child-directed adult speech. We also fit linear mixed-effects models with the rate of semantic change in the diachronic representations and word frequencies. This allows us to inspect the role of word frequencies towards lexical development in children. Further, we perform a qualitative analysis of the diachronic representations from our model, which reveals the categorization and word associations in the mental lexicon of children.","tags":["NLP","Cognitive Science","Psychology"],"title":"Using Diachronic Distributed Word Representations as Models of Lexical Development in Children","type":"publication"},{"authors":["Rajaswa Patil","Jasleen Dhillon","Siddhant Mahurkar","Saumitra Kulkarni","Manav Malhotra","Veeky Baths"],"categories":null,"content":"","date":1611945000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611945000,"objectID":"52b7732309e8b450dfa5406873395834","permalink":"https://rajaswa.github.io/publication/vyakarana-2021/","publishdate":"2021-01-30T00:00:00+05:30","relpermalink":"/publication/vyakarana-2021/","section":"publication","summary":"While there has been significant progress towards developing NLU resources for Indic languages, syntactic evaluation has been relatively less explored. Unlike English, Indic languages have rich morphosyntax, grammatical genders, free linear word-order, and highly inflectional morphology. In this paper, we introduce Vyākarana: a benchmark of gender-balanced Colorless Green sentences in Indic languages for syntactic evaluation of multilingual language models. The benchmark comprises four syntax-related tasks: PoS Tagging, Syntax Tree-depth Prediction, Grammatical Case Marking, and Subject-Verb Agreement. We use the datasets from the evaluation tasks to probe five multilingual language models of varying architectures for syntax in Indic languages. Due to its prevalence, we also include a code-switching setting in our experiments. Our results show that the token-level and sentence-level representations from the Indic language models (IndicBERT and MuRIL) do not capture the syntax in Indic languages as efficiently as the other highly multilingual language models. Further, our layer-wise probing experiments reveal that while mBERT, DistilmBERT, and XLM-R localize the syntax in middle layers, the Indic language models do not show such syntactic localization.","tags":["Indic-NLP","Syntax","Interpretability"],"title":"Vyākarana: A Colorless Green Benchmark for Syntactic Evaluation in Indic Languages","type":"publication"},{"authors":["Rajaswa Patil","Yaman Kumar Singla","Rajiv Ratn Shah","Mika Hama","Roger Zimmermann"],"categories":null,"content":"","date":1609439400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609439400,"objectID":"bd9e82500f8a29ee9d5ed8ceffbb4bc5","permalink":"https://rajaswa.github.io/publication/spoken_discourse_coherence-2021/","publishdate":"2021-01-01T00:00:00+05:30","relpermalink":"/publication/spoken_discourse_coherence-2021/","section":"publication","summary":"While there has been significant progress towards modelling coherence in written discourse, the work in modelling spoken discourse coherence has been quite limited. Unlike the coherence in text, coherence in spoken discourse is also dependent on the prosodic and acoustic patterns in speech. In this paper, we model coherence in spoken discourse with audio-based coherence models. We perform experiments with four coherence-related tasks with spoken discourses. In our experiments, we evaluate machine-generated speech against the speech delivered by expert human speakers. We also compare the spoken discourses generated by human language learners of varying language proficiency levels. Our results show that incorporating the audio modality along with the text benefits the coherence models in performing downstream coherence related tasks with spoken discourses.","tags":["NLP","Speech"],"title":"Towards Modelling Coherence in Spoken Discourse","type":"publication"},{"authors":null,"categories":null,"content":"I\u0026rsquo;ll be participating in a round table discussion at the International CCCP Symposium 2020. I\u0026rsquo;ll be talking about the current state of data and resources in the study of Syntactic Processing!\n","date":1607212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607212800,"objectID":"525665083a70758ea5ec4a7ab9e3debb","permalink":"https://rajaswa.github.io/news/ibrainroundtable/","publishdate":"2020-12-06T00:00:00Z","relpermalink":"/news/ibrainroundtable/","section":"news","summary":"I\u0026rsquo;ll be participating in a round table discussion at the International CCCP Symposium 2020. I\u0026rsquo;ll be talking about the current state of data and resources in the study of Syntactic Processing!","tags":null,"title":"I'll be participating in a round table discussion at the International CCCP Symposium 2020!","type":"news"},{"authors":null,"categories":null,"content":"I\u0026rsquo;ll be attending COLING 2020 to present our work on Humor Grading and Counterfactual Detection! Shoot me an e-mail or catch me at the conference socials!\nThis conference visit is funded by the Cognitive Neuroscience Lab at BITS Goa.\n","date":1607126400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607126400,"objectID":"128302e92f11e87e0c089e445661729b","permalink":"https://rajaswa.github.io/news/coling2020/","publishdate":"2020-12-05T00:00:00Z","relpermalink":"/news/coling2020/","section":"news","summary":"I\u0026rsquo;ll be attending COLING 2020 to present our work on Humor Grading and Counterfactual Detection! Shoot me an e-mail or catch me at the conference socials!\nThis conference visit is funded by the Cognitive Neuroscience Lab at BITS Goa.","tags":null,"title":"I'll be attending COLING 2020 to present our work on Humor Grading and Counterfactual Detection!","type":"news"},{"authors":["Rajaswa Patil","Somesh Singh","Swati Agarwal"],"categories":null,"content":"","date":1606761000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606761000,"objectID":"af19c65db8acdb4040658ec5c51bcab3","permalink":"https://rajaswa.github.io/publication/semeval-propaganda-2020/","publishdate":"2020-12-01T00:00:00+05:30","relpermalink":"/publication/semeval-propaganda-2020/","section":"publication","summary":"Propaganda spreads the ideology and beliefs of like-minded people, brainwashing their audiences, and sometimes leading to violence. SemEval 2020 Task-11 aims to design automated systems for news propaganda detection. Task-11 consists of two sub-tasks, namely, Span Identification - given any news article, the system tags those specific fragments which contain at least one propaganda technique; and Technique Classification - correctly classify a given propagandist statement amongst 14 propaganda techniques. For sub-task 1, we use contextual embeddings extracted from pre-trained transformer models to represent the text data at various granularities and propose a multi-granularity knowledge sharing approach. For sub-task 2, we use an ensemble of BERT and logistic regression classifiers with linguistic features. Our results reveal that the linguistic features are the strong indicators for covering minority classes in a highly imbalanced dataset.","tags":["NLP","AI4SG","Social Computing","Information Retrieval"],"title":"BPGC at SemEval-2020 Task 11: Propaganda Detection in News Articles with Multi-Granularity Knowledge Sharing and Linguistic Features Based Ensemble Learning","type":"publication"},{"authors":["Rajaswa Patil","Veeky Baths"],"categories":null,"content":"","date":1606761000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606761000,"objectID":"0a706038cb88bb531f6631dc4ad97aea","permalink":"https://rajaswa.github.io/publication/semeval-counterfactual-2020/","publishdate":"2020-12-01T00:00:00+05:30","relpermalink":"/publication/semeval-counterfactual-2020/","section":"publication","summary":"In this paper, we describe an approach for modelling causal reasoning in natural language by detecting counterfactuals in text using multi-head self-attention weights. We use pre-trained transformer models to extract contextual embeddings and self-attention weights from the text. We show the use of convolutional layers to extract task-specific features from these self-attention weights. Further, we describe a fine-tuning approach with a common base model for knowledge sharing between the two closely related sub-tasks for counterfactual detection. We analyze and compare the performance of various transformer models in our experiments. Finally, we perform a qualitative analysis with the multi-head self-attention weights to interpret our models’ dynamics.","tags":["NLP","Information Retrieval"],"title":"CNRL at SemEval-2020 Task 5: Modelling Causal Reasoning in Language with Multi-Head Self-Attention Weights Based Counterfactual Detection","type":"publication"},{"authors":["Siddhant Mahurkar","Rajaswa Patil"],"categories":null,"content":"","date":1606761000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606761000,"objectID":"d2a91774b7bad44c659dc1eb7d4ae81a","permalink":"https://rajaswa.github.io/publication/semeval-humor-2020/","publishdate":"2020-12-01T00:00:00+05:30","relpermalink":"/publication/semeval-humor-2020/","section":"publication","summary":"In this paper, we assess the ability of BERT and its derivative models (RoBERTa, DistilBERT, and ALBERT) for short-edits based humor grading. We test these models for humor grading and classification tasks on the Humicroedit and the FunLines dataset. We perform extensive experiments with these models to test their language modeling and generalization abilities via zero-shot inference and cross-dataset inference based approaches. Further, we also inspect the role of self-attention layers in humor-grading by performing a qualitative analysis over the self-attention weights from the final layer of the trained BERT model. Our experiments show that all the pre-trained BERT derivative models show significant generalization capabilities for humor-grading related tasks.","tags":["NLP"],"title":"LRG at SemEval-2020 Task 7: Assessing the Ability of BERT and Derivative Models to Perform Short-Edits Based Humor Grading","type":"publication"},{"authors":null,"categories":null,"content":"\u0026ldquo;I have been shortlisted to attend the first Google Research AI Summer School (2020)! Shoot me an e-mail or catch me at the conference socials!\n","date":1597536000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597536000,"objectID":"821400715dca2293b4a144d854b324ee","permalink":"https://rajaswa.github.io/news/googlesummerschool2020/","publishdate":"2020-08-16T00:00:00Z","relpermalink":"/news/googlesummerschool2020/","section":"news","summary":"\u0026ldquo;I have been shortlisted to attend the first Google Research AI Summer School (2020)! Shoot me an e-mail or catch me at the conference socials!","tags":null,"title":"I'll be attending the Google Research AI Summer School 2020!","type":"news"},{"authors":null,"categories":null,"content":"Our paper CNRL at SemEval-2020 Task 5: Modelling Causal Reasoning in Language with Multi-Head Self-Attention Weights Based Counterfactual Detection has been accpeted for publication at the International Workshop on Semantic Evaluation, 2020.\n","date":1593907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593907200,"objectID":"70d6656fe6bbaea59cefb8a84bd25cad","permalink":"https://rajaswa.github.io/news/semevaltask5accept/","publishdate":"2020-07-05T00:00:00Z","relpermalink":"/news/semevaltask5accept/","section":"news","summary":"Our paper CNRL at SemEval-2020 Task 5: Modelling Causal Reasoning in Language with Multi-Head Self-Attention Weights Based Counterfactual Detection has been accpeted for publication at the International Workshop on Semantic Evaluation, 2020.","tags":null,"title":"Paper on Counterfactual Detection accepted at SemEval 2020!","type":"news"},{"authors":null,"categories":null,"content":"Our paper LRG at SemEval-2020 Task 7: Assessing the Ability of BERT and Derivative Models to Perform Short-Edits Based Humor Grading has been accpeted for publication at the International Workshop on Semantic Evaluation, 2020.\n","date":1593907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593907200,"objectID":"ca6b19a7b316353f9d2ba67d42621275","permalink":"https://rajaswa.github.io/news/semevaltask7accept/","publishdate":"2020-07-05T00:00:00Z","relpermalink":"/news/semevaltask7accept/","section":"news","summary":"Our paper LRG at SemEval-2020 Task 7: Assessing the Ability of BERT and Derivative Models to Perform Short-Edits Based Humor Grading has been accpeted for publication at the International Workshop on Semantic Evaluation, 2020.","tags":null,"title":"Paper on Humor Grading accepted at SemEval 2020!","type":"news"},{"authors":null,"categories":null,"content":"Our paper BPGC at SemEval-2020 Task 11: Propaganda Detection in News Articles with Multi-Granularity Knowledge Sharing and Linguistic Features Based Ensemble Learning has been accpeted for publication at the International Workshop on Semantic Evaluation, 2020.\n","date":1593907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593907200,"objectID":"d9fe938dc21bb801fabba998ceadb074","permalink":"https://rajaswa.github.io/news/semevaltask11accept/","publishdate":"2020-07-05T00:00:00Z","relpermalink":"/news/semevaltask11accept/","section":"news","summary":"Our paper BPGC at SemEval-2020 Task 11: Propaganda Detection in News Articles with Multi-Granularity Knowledge Sharing and Linguistic Features Based Ensemble Learning has been accpeted for publication at the International Workshop on Semantic Evaluation, 2020.","tags":null,"title":"Paper on Propaganda Detection accepted at SemEval 2020!","type":"news"},{"authors":null,"categories":null,"content":"I\u0026rsquo;ll be taking up the Teaching Assitantship duty for the Neuroscience \u0026amp; AI Reading Course at the Cognitive Neuroscience Lab at BITS Goa. We\u0026rsquo;ll be covering the following topics: Syntactic Processing, Grounded Language Learning, Models of Mental Lexicon Development, Neural Correlates of Discourse Coherence, and Brain Embeddings. Shoot me an e-mail to enroll in the course!\n","date":1591315200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591315200,"objectID":"3b707a46674eb3cf017b6e0c0f7d5df6","permalink":"https://rajaswa.github.io/news/neuroairc/","publishdate":"2020-06-05T00:00:00Z","relpermalink":"/news/neuroairc/","section":"news","summary":"I\u0026rsquo;ll be taking up the Teaching Assitantship duty for the Neuroscience \u0026amp; AI Reading Course at the Cognitive Neuroscience Lab at BITS Goa. We\u0026rsquo;ll be covering the following topics: Syntactic Processing, Grounded Language Learning, Models of Mental Lexicon Development, Neural Correlates of Discourse Coherence, and Brain Embeddings.","tags":null,"title":"Neuroscience \u0026 AI Reading Course starts at CNRL BITS Goa!","type":"news"},{"authors":["Rajaswa Patil","Siddhant Mahurkar"],"categories":null,"content":"","date":1576089000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576089000,"objectID":"97a43b49ee45dd2de71b81525aaebcfa","permalink":"https://rajaswa.github.io/publication/kedl-citta-2019/","publishdate":"2019-12-12T00:00:00+05:30","relpermalink":"/publication/kedl-citta-2019/","section":"publication","summary":"Most of the recommendation and search frameworks in Digital Libraries follow a keyword-based approach to resolve text-based search queries. Keyword-based methods usually fail to capture the semantic aspects of the user’s query and often lead to a misleading set of results. In this work, we propose an efficient and content-sentiment aware semantic recommendation framework, Citta. The framework is designed with the BERT language model. It is designed to retrieve semantically related reading recommendations with short input queries and shorter response times. We test the proposed framework on the CMU Book Summary Dataset and discuss the observed advantages and shortcomings of the framework.","tags":["NLP","Information Retrieval"],"title":"Citta: A Lite Semantic Recommendation Framework for Digital Libraries","type":"publication"},{"authors":["Ajay Subramanian","Rajaswa Patil","Veeky Baths"],"categories":null,"content":"","date":1576089000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576089000,"objectID":"e60e9dd094f8f8cd0fe3294c4ae4a18f","permalink":"https://rajaswa.github.io/publication/accs-word2brain2image-2019/","publishdate":"2019-12-12T00:00:00+05:30","relpermalink":"/publication/accs-word2brain2image-2019/","section":"publication","summary":"Recent work in cognitive neuroscience has aimed to better understand how the brain responds to external stimuli. Extensive study is being done to gauge the involvement of various regions of the brain in the processing of external stimuli. A study by Ostarek et al. has produced experimental evidence of the involvement of low-level visual representations in spoken word processing, using Continuous Flash Suppression (CFS). For example, hearing the word ‘car’ induces a visual representation of a car in extrastriate areas of the visual cortex that seems to have a spatial resolution of some kind. Though the structure of these areas of the brain has been extensively studied, research hasn’t really delved into the functional aspects. In this work, we aim to take this a step further by experimenting with generative models such as Variational Autoencoders (VAEs) (Kingma et al 2013) and Generative Adversarial Networks (GANs) (Goodfellow et al. 2014) to generate images purely from the EEG signals induced by listening to spoken words of objects.","tags":["Cognitive Science","Neuroscience"],"title":"Word2Brain2Image: Visual Reconstruction from Spoken Word Representations","type":"publication"}]