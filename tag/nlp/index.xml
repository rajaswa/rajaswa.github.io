<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP | Rajaswa Patil</title>
    <link>https://rajaswa.github.io/tag/nlp/</link>
      <atom:link href="https://rajaswa.github.io/tag/nlp/index.xml" rel="self" type="application/rss+xml" />
    <description>NLP</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© Rajaswa Patil (2022)</copyright><lastBuildDate>Wed, 30 Jun 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://rajaswa.github.io/media/icon_hu1664b71f924a7adcdc24304f9f1a3672_78236_512x512_fill_lanczos_center_2.png</url>
      <title>NLP</title>
      <link>https://rajaswa.github.io/tag/nlp/</link>
    </image>
    
    <item>
      <title>DRIFT</title>
      <link>https://rajaswa.github.io/project/drift/</link>
      <pubDate>Wed, 30 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://rajaswa.github.io/project/drift/</guid>
      <description>&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#about&#34;&gt;About&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#setup&#34;&gt;Setup&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#data&#34;&gt;Data&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#usage&#34;&gt;Usage&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#launch-the-app&#34;&gt;Launch the app&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#train-mode&#34;&gt;Train Mode&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#analysis-mode&#34;&gt;Analysis Mode&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://github.com/rajaswa/DRIFT/raw/main/misc/images/logo_svg.svg&#34; alt=&#34;Logo&#34; height=12.5% width=12.5%/&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;about&#34;&gt;About&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;DRIFT is a tool for Diachronic Analysis of Scientific Literature&lt;/strong&gt;. The application offers &lt;strong&gt;user-friendly&lt;/strong&gt; and &lt;strong&gt;customizable utilities&lt;/strong&gt; for two modes: &lt;strong&gt;&lt;a href=&#34;https://github.com/rajaswa/DRIFT#train-mode&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Training&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&#34;https://github.com/rajaswa/DRIFT#analysis-mode&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Analysis&lt;/a&gt;&lt;/strong&gt;. Currently, the application supports customizable training of diachronic word embeddings with the &lt;strong&gt;&lt;a href=&#34;https://github.com/valedica/twec&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TWEC model&lt;/a&gt;&lt;/strong&gt;. The application supports a variety of analysis methods to monitor &lt;strong&gt;trends and patterns of development in scientific literature&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rajaswa/DRIFT#word-cloud&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Word Cloud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rajaswa/DRIFT#productivityfrequency-plot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Productivity/Frequency Plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rajaswa/DRIFT#acceleration-plot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Acceleration Plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rajaswa/DRIFT#semantic-drift&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Semantic Drift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rajaswa/DRIFT#tracking-clusters&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tracking Clusters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rajaswa/DRIFT#acceleration-heatmap&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Acceleration Heatmap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rajaswa/DRIFT#track-trends-with-similarity&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Track Trends with Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rajaswa/DRIFT#keyword-visualisation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Keyword Visualisation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rajaswa/DRIFT#lda-topic-modelling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LDA Topic Modelling&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: The &lt;a href=&#34;https://share.streamlit.io/gchhablani/drift/main/app.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;online demo&lt;/a&gt; is hosted using &lt;a href=&#34;https://docs.streamlit.io/en/stable/deploy_streamlit_app.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Streamlit sharing&lt;/a&gt;. This is a single-instance single-process deployment, accessible to all the visitors publicly &lt;strong&gt;(avoid sharing sensitive information on the demo)&lt;/strong&gt;. Hence, &lt;strong&gt;it is highly recommended to use your own independent local deployment of the application for a seamless and private experience. One can alternatively fork this repository and host it using &lt;a href=&#34;https://docs.streamlit.io/en/stable/deploy_streamlit_app.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Streamlit sharing&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We would love to know about any issues found on this repository. Please submit an &lt;a href=&#34;https://github.com/rajaswa/DRIFT/issues/new/choose&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;issue&lt;/a&gt; for any query, or contact us &lt;a href=&#34;mailto:sharmabhee@gmail.com&#34;&gt;here&lt;/a&gt;. If you use this application in your work, you can cite this repository and the paper &lt;a href=&#34;https://github.com/rajaswa/DRIFT#citation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;Clone the repository:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/rajaswa/DRIFT.git
cd DRIFT
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Install the requirements:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make install_req
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;The dataset we have used for our demo, and the analysis in the paper was scraped using the &lt;code&gt;arXiv API&lt;/code&gt; (see &lt;a href=&#34;https://github.com/rajaswa/DRIFT/blob/main/crawl_arxiv.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;script&lt;/a&gt;). We scraped papers from the &lt;code&gt;cs.CL&lt;/code&gt; subject. This dataset is available &lt;a href=&#34;https://drive.google.com/drive/folders/1boRFknjKieEVWxansaoMTO_3YbMf6kr9?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The user can upload their own dataset to the &lt;strong&gt;DRIFT&lt;/strong&gt; application. The unprocessed dataset should be present in the following format (as a JSON file):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{
   &amp;lt;year_1&amp;gt;:[
      &amp;lt;paper_1&amp;gt;,
      &amp;lt;paper_2&amp;gt;,
      ...
   ],
   &amp;lt;year_2&amp;gt;:[
      &amp;lt;paper_1&amp;gt;,
      &amp;lt;paper_2&amp;gt;,
      ...
   ],
   ...,
   &amp;lt;year_m&amp;gt;:[
      &amp;lt;paper_1&amp;gt;,
      &amp;lt;paper_2&amp;gt;,
      ...
   ],
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;year_x&lt;/code&gt; is a string (e.g., &lt;code&gt;&amp;quot;1998&amp;quot;&lt;/code&gt;), and &lt;code&gt;paper_x&lt;/code&gt; is a dictionary. An example is given below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{
   &amp;quot;url&amp;quot;:&amp;quot;http://arxiv.org/abs/cs/9809020v1&amp;quot;,
   &amp;quot;date&amp;quot;:&amp;quot;1998-09-15 23:49:32+00:00&amp;quot;,
   &amp;quot;title&amp;quot;:&amp;quot;Linear Segmentation and Segment Significance&amp;quot;,
   &amp;quot;authors&amp;quot;:[
      &amp;quot;Min-Yen Kan&amp;quot;,
      &amp;quot;Judith L. Klavans&amp;quot;,
      &amp;quot;Kathleen R. McKeown&amp;quot;
   ],
   &amp;quot;abstract&amp;quot;:&amp;quot;We present a new method for discovering a segmental discourse structure of a\ndocument while categorizing segment function. We demonstrate how retrieval of\nnoun phrases and pronominal forms, along with a zero-sum weighting scheme,\ndetermines topicalized segmentation. Futhermore, we use term distribution to\naid in identifying the role that the segment performs in the document. Finally,\nwe present results of evaluation in terms of precision and recall which surpass\nearlier approaches.&amp;quot;,
   &amp;quot;journal ref&amp;quot;:&amp;quot;Proceedings of 6th International Workshop of Very Large Corpora\n  (WVLC-6), Montreal, Quebec, Canada: Aug. 1998. pp. 197-205&amp;quot;,
   &amp;quot;category&amp;quot;:&amp;quot;cs.CL&amp;quot;
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The only important key is &lt;code&gt;&amp;quot;abstract&amp;quot;&lt;/code&gt;, which has the raw text. The user can name this key differently. See the &lt;code&gt;Training&lt;/code&gt; section below for more details.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;h3 id=&#34;launch-the-app&#34;&gt;Launch the app&lt;/h3&gt;
&lt;p&gt;To launch the app, run the following command from the terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;streamlit run app.py

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;train-mode&#34;&gt;Train Mode&lt;/h3&gt;
&lt;h4 id=&#34;preprocesing&#34;&gt;Preprocesing&lt;/h4&gt;
&lt;p&gt;The preprocessing stage takes a JSON file structured as shown in the &lt;a href=&#34;https://github.com/rajaswa/DRIFT#data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data&lt;/a&gt; section. They key for raw text is provided on which preprocessing takes place. During the preprocessing of the text, year-wise text files are created in a desired directory. During the preprocessing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All html tags are removed from the text.&lt;/li&gt;
&lt;li&gt;Contractions are replaced (e.g. &amp;lsquo;don&amp;rsquo;t&amp;rsquo; is converted to &amp;lsquo;do not&amp;rsquo;)&lt;/li&gt;
&lt;li&gt;Punctuations, non-ascii characters, stopwords are removed.&lt;/li&gt;
&lt;li&gt;All verbs are lemmatized.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After this, each processed text is stored in the respective year file separated by a new-line, along with all the data in a single file as &lt;code&gt;compass.txt&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;training&#34;&gt;Training&lt;/h4&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://drive.google.com/uc?export=view&amp;id=1TKic-Z50mGC0JBMqc04RFkT1Shsnmpj-&#34; alt=&#34;word_cloud_usage&#34; height=65% width=65%/&gt;
&lt;/p&gt;
&lt;p&gt;The training mode uses the path where the processed text files are stored, and trains the TWEC model on the given text. The TWEC model trains a Word2Vec model on &lt;code&gt;compass.txt&lt;/code&gt; and then the respective time-slices are trained on this model to get corresponding word vectors. In the sidebar, we provide several options like - whether to use Skipgram over CBOW, number of dynamic iterations for training, number of static iterations for training, negative sampling, etc. After training, we store the models at the specified path, which are used later in the analysis.&lt;/p&gt;
&lt;h3 id=&#34;analysis-mode&#34;&gt;Analysis Mode&lt;/h3&gt;
&lt;h4 id=&#34;word-cloud&#34;&gt;Word Cloud&lt;/h4&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://drive.google.com/uc?export=view&amp;id=13v5KHWjn_JXylf1xrYRgCUGTlCM5RyTx&#34; alt=&#34;word_cloud_usage&#34; height=65% width=65%/&gt;
&lt;/p&gt;
&lt;p&gt;A word cloud, or tag cloud, is a textual data visualization which allows anyone to see in a single glance the words which have the highest frequency within a given body of text. Word clouds are typically used as a tool for processing, analyzing and disseminating qualitative sentiment data.&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=6758829&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Word Cloud Explorer: Text Analytics based on Word Clouds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wordcloud Package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.freewordcloudgenerator.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Free Word Cloud Generator&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;productivityfrequency-plot&#34;&gt;Productivity/Frequency Plot&lt;/h4&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://drive.google.com/uc?export=view&amp;id=1YrKsMmw9MmgrzPI47c5Y-GVk4gMCPPoy&#34; alt=&#34;prod_freq_usage&#34; height=65% width=65%/&gt;
&lt;/p&gt;
&lt;p&gt;Our main reference for this method is &lt;a href=&#34;https://www.aclweb.org/anthology/W16-2101.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper&lt;/a&gt;.
In short, this paper uses normalized term frequency and term producitvity as their measures.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Term Frequency&lt;/strong&gt;: This is the normalized frequency of a given term in a given year.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Term Productivity&lt;/strong&gt;: This is a measure of the ability of the concept to produce new multi-word terms. In our case we use bigrams. For each year &lt;em&gt;y&lt;/em&gt; and single-word term &lt;em&gt;t&lt;/em&gt;, and associated &lt;em&gt;n&lt;/em&gt; multi-word terms &lt;em&gt;m&lt;/em&gt;, the productivity is given by the entropy:&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://latex.codecogs.com/svg.latex?%5Csmall%20e%28t%2Cy%29%20%3D%20-%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20%5Clog_%7B2%7D%28p_%7Bm_%7Bi%7D%2Cy%7D%29.p_%7Bm_%7Bi%7D%2Cy%7D&#34; alt=&#34;prod_plot_1&#34;/&gt;
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://latex.codecogs.com/svg.latex?%5Csmall%20p_%7Bm%2Cy%7D%20%3D%20%5Cfrac%7Bf%28m%29%7D%7B%5Csum_%7Bi%3D1%7D%5E%7Bn%7Df%28m_%7Bi%7D%29%7D&#34; alt=&#34;prod_plot_2&#34;/&gt;
&lt;/p&gt;
&lt;p&gt;Based on these two measures, they hypothesize three kinds of terms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Growing Terms&lt;/strong&gt;: Those which have increasing frequency and productivity in the recent years.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Consolidated Terms&lt;/strong&gt;: Those that are growing in frequency, but not in productivity.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Terms in Decline&lt;/strong&gt;: Those which have reached an upper bound of productivity and are being used less in terms of frequency.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, they perform clustering of the terms based on their frequency and productivity curves over the years to test their hypothesis.
They find that the clusters formed show similar trends as expected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: They also evaluate quality of their clusters using pseudo-labels, but we do not use any automated labels here. They also try with and without double-counting multi-word terms, but we stick to double-counting. They suggest it is more explanable.&lt;/p&gt;
&lt;h4 id=&#34;acceleration-plot&#34;&gt;Acceleration Plot&lt;/h4&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://drive.google.com/uc?export=view&amp;id=1aldBPCyqHJjG67tTAYzFXLuGhzpzopZb&#34; alt=&#34;acceleration_plot_usage&#34; height=65% width=65%/&gt;
&lt;/p&gt;
&lt;p&gt;This plot is based on the word-pair acceleration over time. Our inspiration for this method is &lt;a href=&#34;https://sci-hub.se/10.1109/ijcnn.2019.8852140&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper&lt;/a&gt;.
Acceleration is a metric which calculates how quickly the word embeddings for a pair of word get close together or farther apart. If they are getting closer together, it means these two terms have started appearing more frequently in similar contexts, which leads to similar embeddings.
In the paper, it is described as:&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://latex.codecogs.com/svg.latex?%5Csmall%20acceleration%28w_%7Bi%7D%2C%20w_%7Bj%7D%29%20%3D%20sim%28w_%7Bi%7D%2C%20w_%7Bj%7D%29%5E%7Bt&amp;plus;1%7D%20-%20sim%28w_%7Bi%7D%2C%20w_%7Bj%7D%29%5E%7Bt%7D&#34; alt=&#34;acc_plot_1&#34;/&gt;
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://latex.codecogs.com/svg.latex?%5Csmall%20sim%28w_%7Bi%7D%2C%20w_%7Bj%7D%29%20%3D%20cosine%20%28u_%7Bw_%7Bi%7D%7D%2C%20u_%7Bw_%7Bj%7D%7D%29%20%3D%20%5Cfrac%7Bu_%7Bw_%7Bi%7D%7D.u_%7Bw_%7Bj%7D%7D%7D%7B%5Cleft%5ClVert%20u_%7Bw_%7Bi%7D%7D%5Cright%5CrVert%20.%20%5Cleft%5ClVert%20u_%7Bw_%7Bj%7D%7D%5Cright%5CrVert%7D&#34; alt=&#34;acc_plot_2&#34;/&gt;
&lt;/p&gt;
&lt;p&gt;Below, we display the top few pairs between the given start and end year in  dataframe, then one can select years and then select word-pairs in the plot parameters expander. A reduced dimension plot is displayed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: They suggest using skip-gram method over CBOW for the model. They use t-SNE representation to view the embeddings. But their way of aligning the embeddings is different. They also use some stability measure to find the best Word2Vec model. The also use &lt;em&gt;Word2Phrase&lt;/em&gt; which we are planning to add soon.&lt;/p&gt;
&lt;h4 id=&#34;semantic-drift&#34;&gt;Semantic Drift&lt;/h4&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://drive.google.com/uc?export=view&amp;id=1hwltBpJh3fVEA0sR7Zi4GRINr9Wrq_y8&#34; alt=&#34;semantic_drift_usage&#34; height=65% width=65%/&gt;
&lt;/p&gt;
&lt;p&gt;This plot represents the change in meaning of a word over time. This shift is represented on a 2-dimensional representation of the embedding space.
To find the drift of a word, we calculate the distance between the embeddings of the word in the final year and in the initial year. We find the drift for all words and sort them in descending order to find the most drifted words.
We give an option to use one of two distance metrics: Euclidean Distance and Cosine Distance.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://latex.codecogs.com/svg.latex?%5Csmall%20euclidean%5C_distance%20%3D%20%5Csqrt%7B%5Cvec%7Bu%7D.%5Cvec%7Bu%7D%20-%202%20%5Ctimes%20%5Cvec%7Bu%7D.%5Cvec%7Bv%7D%20&amp;plus;%20%5Cvec%7Bv%7D.%5Cvec%7Bv%7D%7D&#34; alt=&#34;sem_drift_1&#34;/&gt;
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://latex.codecogs.com/svg.latex?%5Csmall%20cosine%5C_distance%20%3D%201%20-%20%5Cfrac%7B%5Cvec%7Bu%7D.%5Cvec%7Bv%7D%7D%7B%7C%7C%5Cvec%7Bu%7D%7C%7C%7C%7C%5Cvec%7Bv%7D%7C%7C%7D&#34; alt=&#34;sem_drift_2&#34;/&gt;
&lt;/p&gt;
&lt;p&gt;We plot top-K (sim.) most similar words around the two representations of the selected word.&lt;/p&gt;
&lt;p&gt;In the &lt;code&gt;Plot Parameters&lt;/code&gt; expander, the user can select the range of years over which the drift will be computed. He/She can also select the dimensionality reduction method for plotting the embeddings.&lt;/p&gt;
&lt;p&gt;Below the graph, we provide a list of most drifted words (from the top-K keywords). The user can also choose a custom word.&lt;/p&gt;
&lt;h4 id=&#34;tracking-clusters&#34;&gt;Tracking Clusters&lt;/h4&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://drive.google.com/uc?export=view&amp;id=1iZ8BVx3woyKnF9KLxiQMAUyZQL0NcYdC&#34; alt=&#34;track_clusters_usage&#34; height=65% width=65%/&gt;
&lt;/p&gt;
&lt;p&gt;Word meanings change over time. They come closer or drift apart. In a certain year, words are clumped together, i.e., they belong to one cluster. But over time, clusters can break into two/coalesce together to form one. Unlike the previous module which tracks movement of one word at a time, here, we track the movement of clusters.&lt;/p&gt;
&lt;p&gt;We plot the formed clusters for all the years lying in the selected range of years.
&lt;strong&gt;NOTE:&lt;/strong&gt; We give an option to use one of two libraries for clustering: sklearn or faiss. faiss&#39; KMeans implementation is around 10 times faster than sklearn&amp;rsquo;s.&lt;/p&gt;
&lt;h4 id=&#34;acceleration-heatmap&#34;&gt;Acceleration Heatmap&lt;/h4&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://drive.google.com/uc?export=view&amp;id=11xdjnckFuzCgiJ8ihTosAH_tfSeec41c&#34; alt=&#34;acceleration_heatmap_usage&#34; height=65% width=65%/&gt;
&lt;/p&gt;
&lt;p&gt;This plot is based on the word-pair acceleration over time. Our inspiration for this method is &lt;a href=&#34;https://sci-hub.se/10.1109/ijcnn.2019.8852140&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper&lt;/a&gt;.
Acceleration is a metric which calculates how quickly the word embeddings for a pair of word get close together or farther apart. If they are getting closer together, it means these two terms have started appearing more frequently in similar contexts, which leads to similar embeddings.
In the paper, it is described as:&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://latex.codecogs.com/svg.latex?%5Csmall%20acceleration%28w_%7Bi%7D%2C%20w_%7Bj%7D%29%20%3D%20sim%28w_%7Bi%7D%2C%20w_%7Bj%7D%29%5E%7Bt&amp;plus;1%7D%20-%20sim%28w_%7Bi%7D%2C%20w_%7Bj%7D%29%5E%7Bt%7D&#34; alt=&#34;acc_hm_1&#34;/&gt;
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://latex.codecogs.com/svg.latex?%5Csmall%20sim%28w_%7Bi%7D%2C%20w_%7Bj%7D%29%20%3D%20cosine%20%28u_%7Bw_%7Bi%7D%7D%2C%20u_%7Bw_%7Bj%7D%7D%29%20%3D%20%5Cfrac%7Bu_%7Bw_%7Bi%7D%7D.u_%7Bw_%7Bj%7D%7D%7D%7B%5Cleft%5ClVert%20u_%7Bw_%7Bi%7D%7D%5Cright%5CrVert%20.%20%5Cleft%5ClVert%20u_%7Bw_%7Bj%7D%7D%5Cright%5CrVert%7D&#34; alt=&#34;acc_hm_2&#34;/&gt;
&lt;/p&gt;
&lt;p&gt;For all the selected keywords, we display a heatmap, where the brightness of the colour determines the value of the acceleration between that pair, i.e., the brightness is directly proportional to the acceleration value.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: They suggest using skip-gram method over CBOW for the model.&lt;/p&gt;
&lt;h4 id=&#34;track-trends-with-similarity&#34;&gt;Track Trends with Similarity&lt;/h4&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://drive.google.com/uc?export=view&amp;id=1pxDhkDXLE84QH_QhZR2VEbNGM6QlPAxg&#34; alt=&#34;track_trends_usage&#34; height=65% width=65%/&gt;
&lt;/p&gt;
&lt;p&gt;In this method, we wish to chart the trajectory of a word/topic from year 1 to year 2.&lt;/p&gt;
&lt;p&gt;To accomplish this, we allow the user to pick a word from year 1. At the same time, we ask the user to provide the desired stride. We search for the most similar word in the next stride years. We keep doing this iteratively till we reach year 2, updating the word at each step.&lt;/p&gt;
&lt;p&gt;The user has to select a word and click on &lt;code&gt;Generate Dataframe&lt;/code&gt;. This gives a list of most similar words in the next stride years. The user can now iteratively select the next word from the drop-down till the final year is reached.&lt;/p&gt;
&lt;h4 id=&#34;keyword-visualisation&#34;&gt;Keyword Visualisation&lt;/h4&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://drive.google.com/uc?export=view&amp;id=1Ds45T3I05wKod6Gtdjy6tJeJODcTeuGj&#34; alt=&#34;keyword_viz_usage&#34; height=65% width=65%/&gt;
&lt;/p&gt;
&lt;p&gt;Here, we use the &lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0020025519308588&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YAKE Keyword Extraction&lt;/a&gt; method to extract keywords. You can read more about YAKE &lt;a href=&#34;https://amitness.com/keyphrase-extraction/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In our code, we use an &lt;a href=&#34;https://github.com/LIAAD/yake&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;open source implementation&lt;/a&gt; of YAKE.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Yake returns scores which are indirectly proportional to the keyword importance. Hence, we do the following to report the final scores:&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://latex.codecogs.com/svg.latex?%5Csmall%20new%5C_score%20%3D%20%5Cfrac%7B1%7D%7B10%5E%7B5%7D%20%5Ctimes%20yake%5C_score%7D&#34; alt=&#34;keyword_viz&#34;/&gt;
&lt;/p&gt;
&lt;h4 id=&#34;lda-topic-modelling&#34;&gt;LDA Topic Modelling&lt;/h4&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://drive.google.com/uc?export=view&amp;id=1Lsjfm1gwRswB4hrJJ4bGaFGUKbsTN1nd&#34; alt=&#34;lda_usage&#34; height=65% width=65%/&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Latent Dirichlet Allocation&lt;/a&gt; is a generative probabilistic model for an assortment of documents, generally used for topic modelling and extraction. LDA clusters the text data into imaginary topics.&lt;/p&gt;
&lt;p&gt;Every topic can be represented as a probability distribution over ngrams and every document can be represented as a probability distribution over these generated topics.&lt;/p&gt;
&lt;p&gt;We train LDA on a corpus where each document contains the abstracts of a particular year. We express every year as a probability distribution of topics.&lt;/p&gt;
&lt;p&gt;In the first bar graph, we show how a year can be decomposed into topics. The graphs below the first one show a decomposition of the relevant topics.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;You can cite our work as:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{sharma2021drift,
      title={DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature}, 
      author={Abheesht Sharma and Gunjan Chhablani and Harshit Pandey and Rajaswa Patil},
      year={2021},
      eprint={2107.01198},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OR&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Sharma, A., Chhablani, G., Pandey, H., &amp;amp; Patil, R. (2021). DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature.
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature</title>
      <link>https://rajaswa.github.io/publication/drift-2021/</link>
      <pubDate>Wed, 30 Jun 2021 00:00:00 +0530</pubDate>
      <guid>https://rajaswa.github.io/publication/drift-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Feedback and Memory in Transformers</title>
      <link>https://rajaswa.github.io/project/feedback-and-memory-in-transformers/</link>
      <pubDate>Sun, 02 May 2021 00:00:00 +0000</pubDate>
      <guid>https://rajaswa.github.io/project/feedback-and-memory-in-transformers/</guid>
      <description>&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#key-contributions&#34;&gt;Key Contributions&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#feedback-transformer-implementation&#34;&gt;Feedback Transformer Implementation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#solving-cogs-with-feedback-transformer&#34;&gt;Solving COGS with Feedback Transformer&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#sequence-copy--reverse-task&#34;&gt;Sequence Copy &amp;amp; Reverse Task&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citations&#34;&gt;Citations&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#contact&#34;&gt;Contact&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;p&gt;My final project submission for the &lt;a href=&#34;https://sites.google.com/view/meta-learning-2021/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Meta Learning&lt;/a&gt; course at &lt;a href=&#34;https://www.bits-pilani.ac.in/goa/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BITS Goa&lt;/a&gt; (conducted by &lt;a href=&#34;https://www.tcs.com/tcs-research&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TCS Research&lt;/a&gt; &amp;amp; BITS Goa). The project is based on the &lt;a href=&#34;https://arxiv.org/abs/2002.09402&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Feedback Transformer paper&lt;/a&gt;. The paper introduces a feedback mechanism in transformer models by adding a recurrent memory-attention based approach. This helps the transformer model in:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Accessing higher level (layers) representations&lt;/li&gt;
&lt;li&gt;Maintaining a belief state&lt;/li&gt;
&lt;li&gt;Perform a learnable wieghted combined top-down and bottom-up processing&lt;/li&gt;
&lt;li&gt;Decrease compute memory-consumption at inference&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The project can be run as a colab notebook &lt;a href=&#34;https://colab.research.google.com/github/rajaswa/feedback-and-memory-in-transformers/blob/main/Feedback_and_Memory_in_Transformers.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;colab&#34;&gt;&lt;/a&gt; , where the approach given in the paper can be understood in more detail through experimentation. The experiments&#39; Tensorboard logs and a video explanation for the notebook can be found &lt;a href=&#34;https://drive.google.com/drive/folders/1Py81M90OgvPynZZZ78El4rBzuamu6A7d?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;key-contributions&#34;&gt;Key Contributions&lt;/h2&gt;
&lt;p&gt;The key contributions of this project can be listed as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Implementing and Open-sourcing a modular customizable Feedback Transformer Model in PyTorch&lt;/li&gt;
&lt;li&gt;Experimenting the Feedback Transformer Model with COGS Benchmark (Compositional Generalization)&lt;/li&gt;
&lt;li&gt;Implementing the Sequence Copy &amp;amp; Reverse Task from the original Feedback Transformer Paper&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;feedback-transformer-implementation&#34;&gt;Feedback Transformer Implementation&lt;/h3&gt;
&lt;p&gt;The Feedback Transformer Model has been implemented as PyTorch model class in the given notebook. You can adjust the various hyperparameters and turn the feedback ON/OFF in the Encoder and Decoder of the Model independently. Use the model in the following manner:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = FeedbackTransformerModel(
            encoder_feedback = False,   # Disable Feedback Mehancism in the Encoder
            decoder_feedback = True,    # Enable Feedback Mehancism in the Decoder
            memory_context = 8,         # How long to look in the past for Memory-attention
            input_vocab_size = 800,     # Input Vocabulary Size
            output_vocab_size = 800,    # Output Vocabulary Size
            d_model = 128,              # Model Embedding Dimension
            nhead = 8,                  # Number of Heads in Multi-head Cross-attention and Memory-attention
            num_layers = 4,             # Number of Encoder and Decoder blocks
            dim_feedforward = 256,      # Feedforward Dimension
            max_seq_length = 1000,      # Maximum Sequence Length in Data
            dropout = 0.1,              # Model Dropout Probability 
            PAD_IDX = 0,                # PAD Token ID to Mask Padding tokens for Attention
            activation = &amp;quot;gelu&amp;quot;,        # Model Activation Function: &amp;quot;gelu&amp;quot; / &amp;quot;relu&amp;quot;
    )
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;solving-cogs-with-feedback-transformer&#34;&gt;Solving COGS with Feedback Transformer&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/najoungkim/COGS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COGS Benchmark&lt;/a&gt; is a benchmark for evaluating &lt;strong&gt;compositional generalization &amp;amp; reasoning&lt;/strong&gt; in natural language. The COGS task is that of mapping a &lt;strong&gt;natural language sentence to a lambda-expression based semantic logical form&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input_sentence = &amp;quot;The moose wanted to read .&amp;quot;
output_logical_form = &amp;quot;* moose ( x _ 1 ) ; want . agent ( x _ 2 , x _ 1 ) AND want . xcomp ( x _ 2 , x _ 4 ) AND read . agent ( x _ 4 , x _ 1 )&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This can be treated as a &lt;strong&gt;sequence-to-sequence semantic-parsing&lt;/strong&gt; task. What makes this task challenging is its &lt;strong&gt;Generalization test set&lt;/strong&gt;. The following points make it quite challenging:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Novel (unseen in training) Combination of Familiar Primitives and Grammatical Roles&lt;/li&gt;
&lt;li&gt;Novel (unseen in training) Combination Modified Phrases and Grammatical Roles&lt;/li&gt;
&lt;li&gt;Deeper Recursion (results in longer sentences and deeper lingusitic strucutre i.e. parse tree)&lt;/li&gt;
&lt;li&gt;Verb Argument Structure Alternation&lt;/li&gt;
&lt;li&gt;Verb Class Alteration&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can check the &lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.731.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COGS Paper&lt;/a&gt; for more details on the benchmark.&lt;/p&gt;
&lt;p&gt;The COGS dataset can be loaded as a PyTorch-Lightning Module in the following manner:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;datamodule = COGSDataModule(
                        batch_size=128,         # Batch Size for Training 
                        num_workers=2,          # Number of workers for Data Loading
                        use_100=False,          # Whether to use single-exposure or hundred-exposures for pimitives in the training set
                        use_Gen=True            # Whether to use normal test set or generaliztion test set
            )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: &lt;em&gt;The feedback transformer paper does not include this benchmark or any related task. This is the first attempt (to the best of my knowledge) to inspect the effect of incoroporating feedback and memory based architectural biases in solving compositional generalization problem in natural language.&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;results&#34;&gt;Results&lt;/h4&gt;
&lt;p&gt;While the PyTorch-Lightning profiler and Tensorboard logger (included in the notebook) will give a detailed insights into the experiments, here are key metrics to report:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Encoder Feedback&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Decoder Feedback&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Num Parameters&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Validation Accuracy&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Generalization Accuracy&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Total Training time&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Mean Forward time&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Mean Backward time&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Inference time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;False&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;False&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;12.7k&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;69.5%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;65.44%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;193.43 s&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;22.58 ms&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;25.17 ms&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;20.08 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;False&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;True&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;12.3k&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;74.1%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;70.86%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4441.7 s&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;645.08 ms&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1039.30 ms&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;365.49 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;True&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;True&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;12.2k&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;74.4%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;70.69%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;7402.4 s&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;701.85 ms&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1129.4 ms&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;404.65 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: &lt;em&gt;The results are subject to change in hyperparameters and training settings. The above results are obtained from the current settings given in the notebook. The results can be increased significantly by training bigger models for longer times.&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;discussion&#34;&gt;Discussion&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;Validation accuracy&lt;/strong&gt; (roughly equal to the Normal test accuracy) reflects the &lt;strong&gt;&lt;em&gt;Expressivity&lt;/em&gt;&lt;/strong&gt; of the models towards the COGS task
&lt;ul&gt;
&lt;li&gt;Access to higher level representations might help in semantic-parsing by allowing top-down processing&lt;/li&gt;
&lt;li&gt;In general, incorporating feedback gives the model &lt;strong&gt;more expressivity&lt;/strong&gt; with &lt;strong&gt;lesser number of parameters&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Generalization test accuracy&lt;/strong&gt; (usually lower than Validation and Normal test accuracy) reflects the &lt;strong&gt;&lt;em&gt;Compositional Generalization&lt;/em&gt;&lt;/strong&gt; capabilities of the models
&lt;ul&gt;
&lt;li&gt;This needs accurate inference on previously unseen novel linguistic structures and an ability to maintain a belief state for longer contexts&lt;/li&gt;
&lt;li&gt;On an absolute scale, incorporating feedback &lt;strong&gt;increases the Generalization test accuracies&lt;/strong&gt; significantly&lt;/li&gt;
&lt;li&gt;High &lt;em&gt;Expressivity&lt;/em&gt; can lead to poor &lt;em&gt;Compositional Generalization&lt;/em&gt; in Vanilla Transformer models (as reported in the &lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.731.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COGS Paper&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;The Vanilla Transformer model (no feedback) shows a &lt;strong&gt;5.84%&lt;/strong&gt; decrease in accuracy between the Validation and Generalization test set&lt;/li&gt;
&lt;li&gt;Enabling feedback in Decoder reduces the drop in Generalization accuracy to &lt;strong&gt;4.37%&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Enabling feedback in Encoder further reduces the the drop in Generalization accuracy to &lt;strong&gt;4.98%&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sequence-copy--reverse-task&#34;&gt;Sequence Copy &amp;amp; Reverse Task&lt;/h3&gt;
&lt;p&gt;The Sequence Copy &amp;amp; Reverse task is included in the &lt;a href=&#34;https://arxiv.org/abs/2002.09402&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Feedback Transformer paper&lt;/a&gt; as an Algorithmic task to test the role of memory in long-sequence processing. Since the official dataset is not publicly available, we generate the dataset synthetically.&lt;/p&gt;
&lt;p&gt;The sequence copy &amp;amp; reverse dataset can be loaded as a PyTorch-Lightning Module in the following manner:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;datamodule = SequenceCopyDataModule(
    batch_size=64,                  # Batch Size for Training
    num_workers=2,                  # Number of workers for Data Loading
    num_samples_train=10000,        # Number of samples to generate for training set
    num_samples_eval=1000,          # Number of samples to generate for validation and test set
    max_length_train=10,            # Sequence length in training samples
    max_length_eval=50,             # Sequence length in evaluation samples (Should be significantly longer to test for memory effect)
    reverse=True,                   # Whether to Copy the Input Sequence or Reverse the Input Sequence
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: &lt;em&gt;The ablation analysis for this task with Feedback Transformer is still in progress. One can still train the Feedback Transformer for this task using the last section of the project&amp;rsquo;s colab notebook.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;citations&#34;&gt;Citations&lt;/h2&gt;
&lt;p&gt;If you use the code in this repository in any manner, cite the repository:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@misc{patil2021-feedback-github,
    author       = {Rajaswa Patil},
    title        = {feedback-and-memory-in-transformers},
    month        = apr,
    year         = 2021,
    publisher    = {Github},
    url          = &amp;quot;https://github.com/rajaswa/feedback-and-memory-in-transformers&amp;quot;
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you use the code for Feedback Transfomer or the Sequence Copy &amp;amp; Reverse task, cite the Feedback Transformer paper:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@misc{fan2021addressing,
      title={Addressing Some Limitations of Transformers with Feedback Memory}, 
      author={Angela Fan and Thibaut Lavril and Edouard Grave and Armand Joulin and Sainbayar Sukhbaatar},
      year={2021},
      eprint={2002.09402},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you use the code from COGS Benchmark data processing and loading, cite the COGS paper:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@inproceedings{kim-linzen-2020-cogs,
    title = &amp;quot;{COGS}: A Compositional Generalization Challenge Based on Semantic Interpretation&amp;quot;,
    author = &amp;quot;Kim, Najoung  and
      Linzen, Tal&amp;quot;,
    booktitle = &amp;quot;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)&amp;quot;,
    month = nov,
    year = &amp;quot;2020&amp;quot;,
    address = &amp;quot;Online&amp;quot;,
    publisher = &amp;quot;Association for Computational Linguistics&amp;quot;,
    url = &amp;quot;https://www.aclweb.org/anthology/2020.emnlp-main.731&amp;quot;,
    doi = &amp;quot;10.18653/v1/2020.emnlp-main.731&amp;quot;,
    pages = &amp;quot;9087--9105&amp;quot;,
    abstract = &amp;quot;Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96{--}99{\%}), but generalization accuracy was substantially lower (16{--}35{\%}) and showed high sensitivity to random seed (+-6{--}8{\%}). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;contact&#34;&gt;Contact&lt;/h2&gt;
&lt;p&gt;Submit an issue &lt;a href=&#34;https://github.com/rajaswa/feedback-and-memory-in-transformers/issues/new/choose&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Diachronic Distributed Word Representations as Models of Lexical Development in Children</title>
      <link>https://rajaswa.github.io/publication/lexical-development-children/</link>
      <pubDate>Sat, 13 Mar 2021 00:00:00 +0530</pubDate>
      <guid>https://rajaswa.github.io/publication/lexical-development-children/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Towards Modelling Coherence in Spoken Discourse</title>
      <link>https://rajaswa.github.io/publication/spoken_discourse_coherence-2021/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0530</pubDate>
      <guid>https://rajaswa.github.io/publication/spoken_discourse_coherence-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>BPGC at SemEval-2020 Task 11: Propaganda Detection in News Articles with Multi-Granularity Knowledge Sharing and Linguistic Features Based Ensemble Learning</title>
      <link>https://rajaswa.github.io/publication/semeval-propaganda-2020/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0530</pubDate>
      <guid>https://rajaswa.github.io/publication/semeval-propaganda-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CNRL at SemEval-2020 Task 5: Modelling Causal Reasoning in Language with Multi-Head Self-Attention Weights Based Counterfactual Detection</title>
      <link>https://rajaswa.github.io/publication/semeval-counterfactual-2020/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0530</pubDate>
      <guid>https://rajaswa.github.io/publication/semeval-counterfactual-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>LRG at SemEval-2020 Task 7: Assessing the Ability of BERT and Derivative Models to Perform Short-Edits Based Humor Grading</title>
      <link>https://rajaswa.github.io/publication/semeval-humor-2020/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0530</pubDate>
      <guid>https://rajaswa.github.io/publication/semeval-humor-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Citta: A Lite Semantic Recommendation Framework for Digital Libraries</title>
      <link>https://rajaswa.github.io/publication/kedl-citta-2019/</link>
      <pubDate>Thu, 12 Dec 2019 00:00:00 +0530</pubDate>
      <guid>https://rajaswa.github.io/publication/kedl-citta-2019/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
